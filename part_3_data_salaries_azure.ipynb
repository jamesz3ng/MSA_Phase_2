{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Azure Machine Learning SDK core\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Workspace\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      5\u001b[0m \u001b[39m# Scikit-learn and others\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "# Azure Machine Learning SDK core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Scikit-learn and others\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\"Azure_machine_learning/config.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering model onto Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model data_salaries_random_forest_regression\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(ws, model_name=\"data_salaries_random_forest_regression\", model_path=\"model.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and share endpoint for marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[8255.1]'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "data = {\n",
    "  \"data\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://msa2023-phase2-azure-przto.australiaeast.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = 'uk8j9RkV2pEsssu2lBxK2yfwz719CyOz'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "headers = {\n",
    "    'Content-Type':'application/json', \n",
    "    'Authorization':('Bearer '+ api_key), \n",
    "    'azureml-model-deployment': 'data-salaries-random-forest-re-1'\n",
    "}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the request ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Hypertuning parametres "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the accuracy of the predictions of the random forest model for the market segmenetations dataset is not high I want to see if I can tune the hyperparametres further and improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.52.0 to work with MSA2023-Phase2-Azure\n"
     ]
    }
   ],
   "source": [
    "## Connecting the workspace\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "ws = Workspace.from_config(\"Azure_machine_learning/config.json\")\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request failed with: 404 Client Error: Data version market segmentation dataset:None (dataContainerName:version) not found. for url: https://australiaeast.experiments.azureml.net/data/v1.0/subscriptions/08005a95-f54c-4812-929f-fa2746cd2c98/resourceGroups/MSA2023-Phase2-Azure/providers/Microsoft.MachineLearningServices/workspaces/MSA2023-Phase2-Azure/dataversion/market%20segmentation%20dataset/versions/None?includeSavedDatasets=true\n",
      "Tried to retrieve v2 data asset but could not find v2 data assetregistered with name \"market segmentation dataset\" (version: None) in the workspace.\n",
      "2023-08-09 10:29:07.578145 | ActivityCompleted: Activity=get_by_name, HowEnded=Failure, Duration=1299.65 [ms], Info = {'activity_id': 'e1a5fa93-157f-4ba4-ad5f-eeccd3f61247', 'activity_name': 'get_by_name', 'activity_type': 'PublicApi', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.52.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Success', 'durationMs': 0.07}, Exception=UserErrorException; UserErrorException:\n",
      "\tMessage: Cannot find dataset registered with name \"market segmentation dataset\" (version: None) in the workspace.\n",
      "\tInnerException None\n",
      "\tErrorResponse \n",
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"UserError\",\n",
      "        \"message\": \"Cannot find dataset registered with name \\\"market segmentation dataset\\\" (version: None) in the workspace.\"\n",
      "    }\n",
      "}\n",
      "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 10:29:10.147731 | ActivityCompleted: Activity=from_delimited_files, HowEnded=Failure, Duration=0.94 [ms], Info = {'activity_id': '773e3082-b280-444d-bfde-d50dd56fffe1', 'activity_name': 'from_delimited_files', 'activity_type': 'PublicApi', 'app_name': 'TabularDataset', 'source': 'azureml.dataset', 'version': '1.52.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 1299.65}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/Users/james/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/bin/python\" -m pip install azureml-dataset-runtime --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./cleaned_data/market_segmentation_cleaned.csv\n",
      "Uploaded ./cleaned_data/market_segmentation_cleaned.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/Users/james/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/bin/python\" -m pip install azureml-dataset-runtime --upgrade",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m default_ds\u001b[39m.\u001b[39mupload_files(files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m./cleaned_data/market_segmentation_cleaned.csv\u001b[39m\u001b[39m'\u001b[39m], target_path \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmarket-segmentation/\u001b[39m\u001b[39m'\u001b[39m, overwrite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, show_progress \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Creating a tabular dataset from the path on the datastore\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tab_data_set \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mTabular\u001b[39m.\u001b[39;49mfrom_delimited_files(path\u001b[39m=\u001b[39;49m(default_ds, \u001b[39m'\u001b[39;49m\u001b[39mmarket-segmentation/*.csv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     13\u001b[0m \u001b[39m# Register the tabular dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/lib/python3.11/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    133\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(al, \u001b[39m'\u001b[39m\u001b[39mactivity_info\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/lib/python3.11/site-packages/azureml/data/dataset_factory.py:357\u001b[0m, in \u001b[0;36mTabularDatasetFactory.from_delimited_files\u001b[0;34m(path, validate, include_path, infer_column_types, set_column_types, separator, header, partition_format, support_multi_line, empty_as_string, encoding)\u001b[0m\n\u001b[1;32m    347\u001b[0m     dataflow \u001b[39m=\u001b[39m dataprep()\u001b[39m.\u001b[39mread_csv(path,\n\u001b[1;32m    348\u001b[0m                                    verify_exists\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m                                    include_path\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                    empty_as_string\u001b[39m=\u001b[39mempty_as_string,\n\u001b[1;32m    355\u001b[0m                                    encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m    356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     dataflow \u001b[39m=\u001b[39m dataprep()\u001b[39m.\u001b[39mread_csv(path,\n\u001b[1;32m    358\u001b[0m                                    verify_exists\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    359\u001b[0m                                    include_path\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m                                    infer_column_types\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m                                    separator\u001b[39m=\u001b[39mseparator,\n\u001b[1;32m    362\u001b[0m                                    header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    363\u001b[0m                                    quoting\u001b[39m=\u001b[39msupport_multi_line,\n\u001b[1;32m    364\u001b[0m                                    encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m    366\u001b[0m dataflow \u001b[39m=\u001b[39m _transform_and_validate(\n\u001b[1;32m    367\u001b[0m     dataflow, partition_format, include_path,\n\u001b[1;32m    368\u001b[0m     validate,\n\u001b[1;32m    369\u001b[0m     infer_column_types \u001b[39mor\u001b[39;00m _is_inference_required(set_column_types))\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m infer_column_types:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/lib/python3.11/site-packages/azureml/data/_dataprep_helper.py:36\u001b[0m, in \u001b[0;36mdataprep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdataprep\u001b[39m():\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dataprep_installed():\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[1;32m     37\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataprep\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_dprep\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     check_min_version()\n",
      "\u001b[0;31mImportError\u001b[0m: Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/Users/james/Library/CloudStorage/OneDrive-TheUniversityofAuckland/Phase_2/.venv/bin/python\" -m pip install azureml-dataset-runtime --upgrade"
     ]
    }
   ],
   "source": [
    "# Uploaded dataset to azure ws\n",
    "from azureml.core import Dataset\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if 'market segmentation dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['./cleaned_data/market_segmentation_cleaned.csv'], target_path ='market-segmentation/', overwrite = True, show_progress = True)\n",
    "\n",
    "\n",
    "    # Creating a tabular dataset from the path on the datastore\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'market-segmentation/*.csv'))\n",
    "\n",
    "    # Register the tabular dataset\n",
    "\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws,\n",
    "                                            name = 'market segmentation dataset',\n",
    "                                            description='market segmentation data',\n",
    "                                            tags = {'format':'CSV'},\n",
    "                                            create_new_version = True)\n",
    "        print('Dataset Registered')\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print(\"Dataset already registered\")\n",
    "                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "experiment_folder ='market_segmentation_training-hyperdrive'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(\"Folder ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing market_segmentation_training-hyperdrive/market_segmentation_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/market_segmentation_training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Input dataset\n",
    "parser.add_argument(\"--input-data\", type=str, dest='input_data', help='training dataset')\n",
    "\n",
    "# Hyperparameters\n",
    "parser.add_argument('--n_estimators', type=int, dest='n_estimators', default=100, help='number of estimators')\n",
    "parser.add_argument('--max_depth', type=int, dest='max_depth', default=None, help='maximum depth of the tree')\n",
    "\n",
    "# Add arguments to args collection\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('n_estimators',  np.int(args.n_estimators))\n",
    "run.log('max_depth',  np.float(args.max_depth) if args.max_depth else 'None')\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "segmentation_data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels NEED TO EDIT THIS\n",
    "X = segmentation_data.drop('Segment', axis=1)\n",
    "y = segmentation_data['Segment']\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a Random Forest classification model with the specified hyperparameters\n",
    "print('Training a classification model')\n",
    "model = RandomForestClassifier(n_estimators=args.n_estimators, max_depth=args.max_depth).fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# If the segmentation labels are binary (e.g., 0 and 1), we can compute AUC.\n",
    "# If not, you'll need to adjust this or skip AUC computation.\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    run.log('AUC', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/market_segmentation_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress..\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"market-segmentation-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating file to allow a python environment to be hosted on compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting market_segmentation_training-hyperdrive/hyperdrive_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/hyperdrive_env.yml\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/hyperdrive_env.yml\")\n",
    "\n",
    "# Get the training dataset\n",
    "market_segmentation_ds = ws.datasets.get(\"market segmentation dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='market_segmentation_training.py',\n",
    "                                # Add non-hyperparameter arguments -in this case, the training dataset\n",
    "                                arguments = ['--input-data', market_segmentation_ds.as_named_input('training_data')],\n",
    "                                environment=hyper_env,\n",
    "                                compute_target = training_cluster)\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        # Hyperdrive will try 6 combinations, adding these as script arguments\n",
    "        '--n_estimators': choice(10, 50, 100),\n",
    "        '--max_depth': choice(None, 10, 20, 30)  # None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=None,  # No early stopping policy\n",
    "                          primary_metric_name='AUC',  # Assuming you're using AUC as the metric; change if needed\n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=12,  # Adjusted to account for new parameter combinations\n",
    "                          max_concurrent_runs=2)  # Run up to 2 iterations in parallel\n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='mslearn-marketsegmentation-hyperdrive')\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the best performing run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
