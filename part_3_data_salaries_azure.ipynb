{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Machine Learning SDK core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Scikit-learn and others\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\"Azure_machine_learning/config.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering model onto Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model data_salaries_random_forest_regression\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(ws, model_name=\"data_salaries_random_forest_regression\", model_path=\"model.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and share endpoint for marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[8255.1]'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "data = {\n",
    "  \"data\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://msa2023-phase2-azure-przto.australiaeast.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = 'uk8j9RkV2pEsssu2lBxK2yfwz719CyOz'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "headers = {\n",
    "    'Content-Type':'application/json', \n",
    "    'Authorization':('Bearer '+ api_key), \n",
    "    'azureml-model-deployment': 'data-salaries-random-forest-re-1'\n",
    "}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the request ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Hypertuning parametres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the accuracy of the predictions of the random forest model for the market segmenetations dataset is not high I want to see if I can tune the hyperparametres further and improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting the workspace\n",
    "\n",
    "ws = Workspace.from_config(\"Azure_machine_learning/config.json\")\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploaded dataset to azure ws\n",
    "from azureml.core import Dataset\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if 'market segmentation dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(file=['./cleaned_data/market_segmentation_cleaned.csv'], target_path ='market-segmentation/', overwrite = True, show_progress = True)\n",
    "\n",
    "\n",
    "    # Creating a tabular dataset from the path on the datastore\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'market-segmentation/*.csv'))\n",
    "\n",
    "    # Register the tabular dataset\n",
    "\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws,\n",
    "                                            name = 'market segmentation dataset',\n",
    "                                            description='market segmentation data',\n",
    "                                            tags = {'format':'CSV'},\n",
    "                                            create_new_version = True)\n",
    "        print('Dataset Registered')\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print(\"Dataset already registered\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_folder ='market_segmentation_training-hyperdrive'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(\"Folder ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/market_segmentation_training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Input dataset\n",
    "parser.add_argument(\"--input-data\", type=str, dest='input_data', help='training dataset')\n",
    "\n",
    "# Hyperparameters\n",
    "parser.add_argument('--n_estimators', type=int, dest='n_estimators', default=100, help='number of estimators')\n",
    "parser.add_argument('--max_depth', type=int, dest='max_depth', default=None, help='maximum depth of the tree')\n",
    "\n",
    "# Add arguments to args collection\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('n_estimators',  np.int(args.n_estimators))\n",
    "run.log('max_depth',  np.float(args.max_depth) if args.max_depth else 'None')\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "segmentation_data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels NEED TO EDIT THIS\n",
    "X = segmentation_data.drop('Segment', axis=1)\n",
    "y = segmentation_data['Segment']\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a Random Forest classification model with the specified hyperparameters\n",
    "print('Training a classification model')\n",
    "model = RandomForestClassifier(n_estimators=args.n_estimators, max_depth=args.max_depth).fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# If the segmentation labels are binary (e.g., 0 and 1), we can compute AUC.\n",
    "# If not, you'll need to adjust this or skip AUC computation.\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    run.log('AUC', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/market_segmentation_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"market-segmentation-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating file to allow a python environment to be hosted on compute\n",
    "%%writefile $experiment_folder/hyperdrive_env.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/hyperdrive_env.yml\")\n",
    "\n",
    "# Get the training dataset\n",
    "market_segmentation_ds = ws.datasets.get(\"market segmentation dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='market_segmentation_training.py',\n",
    "                                # Add non-hyperparameter arguments -in this case, the training dataset\n",
    "                                arguments = ['--input-data', market_segmentation_ds.as_named_input('training_data')],\n",
    "                                environment=hyper_env,\n",
    "                                compute_target = training_cluster)\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        # Hyperdrive will try 6 combinations, adding these as script arguments\n",
    "        '--n_estimators': choice(10, 50, 100),\n",
    "        '--max_depth': choice(None, 10, 20, 30)  # None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=None,  # No early stopping policy\n",
    "                          primary_metric_name='AUC',  # Assuming you're using AUC as the metric; change if needed\n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=12,  # Adjusted to account for new parameter combinations\n",
    "                          max_concurrent_runs=2)  # Run up to 2 iterations in parallel\n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='mslearn-marketsegmentation-hyperdrive')\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the best performing run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
